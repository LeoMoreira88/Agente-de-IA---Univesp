import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import tempfile
import os
import pandas as pd
import base64
from openai import OpenAI
import re
import json
import uuid
import traceback
from typing import TypedDict, List, Dict, Any, Optional

# Bibliotecas do LangChain para processamento de texto
from langchain.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredPowerPointLoader,
    UnstructuredExcelLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document
from langgraph.graph import StateGraph, END


# Fun√ß√£o que normaliza o nome de um arquivo, removendo espa√ßos extras
def normalizar_nome_arquivo(nome):
    return re.sub(r'\s+', ' ', nome).strip()


# Fun√ß√£o para anonimizar dados sens√≠veis dentro de um texto,
# como CPF, RG e e-mails, substituindo por marcadores gen√©ricos.
def anonimizar_dados(texto):
    # Anonimiza CPF no formato 123.456.789-00
    texto = re.sub(r'\b\d{3}\.?\d{3}\.?\d{3}-?\d{2}\b',
                   '[CPF ANONIMIZADO]', texto)
    # Anonimiza RG (pode variar em quantidade de d√≠gitos, mas segue um padr√£o b√°sico)
    texto = re.sub(r'\b\d{1,2}\.?\d{3}\.?\d{3}-?\d{1}\b',
                   '[RG ANONIMIZADO]', texto)
    # Anonimiza endere√ßos de e-mail
    texto = re.sub(
        r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL ANONIMIZADO]', texto)
    return texto


# Tupla com as extens√µes de arquivo permitidas para upload
EXTENSOES_PERMITIDAS = (
    '.pdf', '.docx', '.pptx', '.xlsx', '.csv',
    '.png', '.jpg', '.jpeg',
    '.mp3', '.mp4', '.mpeg', '.mpga', '.m4a', '.wav', '.webm'
)

# Cria√ß√£o de um cliente para chamadas de API da OpenAI
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])


# Classe usada para tipar o estado do "Agente" (a aplica√ß√£o),
# definindo quais campos s√£o esperados. Isso ajuda no controle de estado.
class AgentState(TypedDict):
    messages: List[Dict[str, Any]]
    vetorstore: Optional[FAISS]
    documents: List[Document]
    file_uploads: List[st.runtime.uploaded_file_manager.UploadedFile]
    memory: ConversationBufferMemory
    using_uploaded: bool
    uploaded_dataframes: Dict[str, Any]


# Fun√ß√£o para processar planilhas CSV, dividindo as linhas em chunks (segmentos)
# e criando objetos Document do LangChain. Cada chunk recebe metadados para refer√™ncia.
def processar_planilha(caminho_arquivo, nome_original, origin='uploaded'):
    try:
        df = pd.read_csv(caminho_arquivo)
        header = df.columns.tolist()
        documentos = []
        chunk_size = 10  # tamanho de cada chunk de linhas

        # Percorre o DataFrame em blocos de 10 linhas
        for i in range(0, len(df), chunk_size):
            chunk = df.iloc[i:i+chunk_size]
            content = f"Colunas: {', '.join(header)}\n\nDados:\n{chunk.to_markdown(index=False)}"
            # Anonimiza dados sens√≠veis de cada trecho
            content = anonimizar_dados(content)
            # Cria um Document do LangChain
            documentos.append(Document(
                page_content=content,
                metadata={
                    'source': nome_original,
                    'tipo': 'planilha',
                    'origin': origin,
                    'linha_inicial': i,
                    'linha_final': i + chunk_size
                }
            ))
        return documentos
    except Exception as e:
        st.error(f"Erro ao processar a planilha CSV {nome_original}: {str(e)}")
        return []


# Fun√ß√£o para processar planilhas no formato XLSX (Excel).
# Ela percorre cada aba (sheet) do Excel e divide os dados em chunks de 10 linhas.
def processar_planilha_xlsx(caminho_arquivo, nome_original, origin='uploaded'):
    try:
        documentos = []
        chunk_size = 10
        with pd.ExcelFile(caminho_arquivo) as xl:
            # Para cada aba (sheet) do arquivo
            for sheet in xl.sheet_names:
                try:
                    df = xl.parse(sheet_name=sheet)

                    # Converte colunas datetime em string para evitar problemas na hora de converter em markdown
                    for col in df.select_dtypes(include=['datetime64']).columns:
                        df[col] = df[col].astype(str)

                    if df.empty:
                        continue

                    header = [str(col) for col in df.columns.tolist()]

                    # Percorre a aba em blocos de 10 linhas
                    for i in range(0, len(df), chunk_size):
                        chunk = df.iloc[i:i+chunk_size]
                        try:
                            markdown_text = chunk.to_markdown(index=False)
                        except Exception as md_error:
                            markdown_text = str(chunk)

                        content = (f"Sheet: {sheet}\n"
                                   f"Colunas: {', '.join(header)}\n\n"
                                   f"Dados:\n{markdown_text}")

                        # Anonimiza dados sens√≠veis
                        content = anonimizar_dados(content)
                        # Cria Document do LangChain para cada bloco
                        documentos.append(Document(
                            page_content=content,
                            metadata={
                                'source': nome_original,
                                'tipo': 'planilha',
                                'origin': origin,
                                'sheet': sheet,
                                'linha_inicial': i,
                                'linha_final': i + chunk_size
                            }
                        ))
                except Exception as sheet_error:
                    st.error(
                        f"Erro ao processar a sheet '{sheet}': {str(sheet_error)}")
                    continue

        return documentos
    except Exception as e:
        st.error(
            f"Erro ao processar a planilha XLSX {nome_original}: {str(e)}")
        return []


# Fun√ß√£o principal para criar o workflow (fluxo de estados) usando o LangGraph.
# Esse workflow gerencia o processamento de arquivos, a recupera√ß√£o de documentos e a gera√ß√£o de respostas.
def create_workflow():
    # Cria√ß√£o de embeddings usando o modelo da OpenAI para indexar trechos de documentos
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

    # Configura√ß√£o do Splitter para dividir documentos em peda√ßos menores (chunks)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # tamanho de cada chunk em caracteres
        chunk_overlap=200,  # sobreposi√ß√£o entre chunks
        separators=["\n\n", "\n", " ", ""]
    )

    # Fun√ß√£o interna do workflow que processa arquivos enviados (upload)
    def process_uploaded_files(state: AgentState):
        if not state['file_uploads']:
            return state

        # Garante que existir√° um dicion√°rio para armazenar DataFrames
        if 'uploaded_dataframes' not in state:
            state['uploaded_dataframes'] = {}

        documentos = []
        # Para cada arquivo enviado
        for arquivo in state['file_uploads']:
            nome_original = arquivo.name
            extensao = os.path.splitext(nome_original)[1].lower()
            try:
                # Salva o arquivo temporariamente
                with tempfile.NamedTemporaryFile(delete=False, suffix=extensao) as temp_file:
                    temp_file.write(arquivo.getvalue())
                    caminho_arquivo = temp_file.name

                # Verifica a extens√£o do arquivo para decidir o tipo de processamento
                if extensao in ('.mp3', '.mp4', '.mpeg', '.mpga', '.m4a', '.wav', '.webm'):
                    # Se for arquivo de √°udio, faz transcri√ß√£o
                    docs = transcrever_audio(
                        caminho_arquivo, nome_original, 'uploaded')
                elif extensao in ('.png', '.jpg', '.jpeg'):
                    # Se for imagem, faz "processamento" (descri√ß√£o usando IA)
                    docs = processar_imagem(caminho_arquivo, 'uploaded')
                elif extensao == '.csv':
                    # Se for CSV, l√™ em DataFrame e armazena, depois processa
                    df = pd.read_csv(caminho_arquivo)
                    state['uploaded_dataframes'][nome_original] = df
                    docs = processar_planilha(
                        caminho_arquivo, nome_original, origin='uploaded')
                elif extensao == '.xlsx':
                    # Se for XLSX, l√™ todas as sheets, armazena e processa
                    with pd.ExcelFile(caminho_arquivo) as xl:
                        df_dict = {sheet: xl.parse(
                            sheet_name=sheet) for sheet in xl.sheet_names}
                    state['uploaded_dataframes'][nome_original] = df_dict
                    docs = processar_planilha_xlsx(
                        caminho_arquivo, nome_original, origin='uploaded')
                else:
                    # Caso seja PDF, DOCX, PPTX, etc. Carrega usando loaders do LangChain
                    loader = {
                        '.pdf': PyPDFLoader,
                        '.docx': Docx2txtLoader,
                        '.pptx': UnstructuredPowerPointLoader,
                    }.get(extensao)
                    if loader:
                        docs = loader(caminho_arquivo).load()
                        for doc in docs:
                            doc.page_content = anonimizar_dados(
                                doc.page_content)
                            doc.metadata.update({
                                'tipo': 'documento',
                                'source': nome_original,
                                'origin': 'uploaded'
                            })
                    else:
                        docs = []

                documentos.extend(docs)
                # Remove o arquivo tempor√°rio
                os.unlink(caminho_arquivo)
            except Exception as e:
                st.error(f"Erro no processamento de {nome_original}: {str(e)}")

        # Se obteve algum documento ap√≥s o processamento, cria o √≠ndice Vetorstore (FAISS)
        if documentos:
            segmentos = text_splitter.split_documents(documentos)
            state['vetorstore'] = FAISS.from_documents(segmentos, embeddings)
            state['documents'] = segmentos.copy()
            state['using_uploaded'] = True

        # Limpa a lista de arquivos para evitar reprocessamento
        state['file_uploads'] = []
        return state

    # Fun√ß√£o para decidir o pr√≥ximo passo no workflow
    # baseado na √∫ltima mensagem e no contexto (ex.: se h√° documentos carregados ou n√£o).
    def route_question(state: AgentState):
        if not state.get('messages'):
            return "generate_direct_answer"
        if state['vetorstore'] is None or len(state['documents']) == 0:
            return "generate_direct_answer"
        last_message = state['messages'][-1].get('content', '')

        # Se o usu√°rio solicitou "transcri√ß√£o completa", envia para rota espec√≠fica
        if any(keyword in last_message.lower() for keyword in ["transcri√ß√£o completa", "transcri√ß√£o do √°udio"]):
            return "handle_special_request"
        return "retrieve_documents"

    # Passo do workflow para lidar com solicita√ß√µes espec√≠ficas,
    # como "trazer a transcri√ß√£o completa do √°udio".
    def handle_special_request(state: AgentState):
        # Filtra documentos que s√£o do tipo "√°udio"
        audio_docs = [doc for doc in state['documents']
                      if doc.metadata.get('tipo') == '√°udio']
        if audio_docs:
            resposta_lines = ["Transcri√ß√µes completas de √°udio:"]
            for doc in audio_docs:
                transcription = doc.page_content.strip()
                resposta_lines.append(
                    f"Fonte: {doc.metadata['source']}\n{transcription}"
                )
            resposta = "\n\n".join(resposta_lines)
        else:
            resposta = "Nenhuma transcri√ß√£o de √°udio encontrada."

        # Adiciona a resposta ao hist√≥rico de mensagens
        state['messages'].append({"role": "assistant", "content": resposta})
        return state

    # Passo do workflow para recuperar documentos relevantes baseados na pergunta do usu√°rio.
    def retrieve_documents(state: AgentState):
        try:
            question = state['messages'][-1]['content']
            if state['vetorstore'] is None:
                raise ValueError(
                    "Nenhum documento foi carregado para consulta")

            # Cria um retriever usando MMR (Maximal Marginal Relevance)
            retriever = state['vetorstore'].as_retriever(
                search_type="mmr",
                search_kwargs={
                    "k": 5,         # n√∫mero de documentos a retornar
                    "fetch_k": 200,  # n√∫mero total de documentos para considerar antes de ranquear
                    "lambda_mult": 0.5,
                }
            )
            docs = retriever.get_relevant_documents(question)
            if not docs:
                st.warning(
                    "‚ö†Ô∏è Nenhum documento relevante encontrado para a consulta")
            return {**state, "documents": docs}
        except Exception as e:
            st.error(f"Erro na recupera√ß√£o de documentos: {str(e)}")
            return state

    # Passo do workflow para gerar a resposta final, usando o modelo ChatOpenAI selecionado.
    def generate_answer(state: AgentState):
        selected_model = st.session_state.get("selected_model", "gpt-4o")

        # Define o modelo de chat com base na escolha do usu√°rio
        if selected_model == "gpt-4o":
            chat_model = ChatOpenAI(model=selected_model, temperature=0)
        else:
            chat_model = ChatOpenAI(model=selected_model, temperature=1)

        if not state.get('messages'):
            st.error("Fa√ßa uma pergunta sobre o arquivo carregado.")
            return state

        question = state['messages'][-1].get('content', '')
        if not question:
            st.error("Pergunta vazia.")
            return state

        resposta = ""
        try:
            if state.get('documents'):
                # Constr√≥i um contexto unindo o conte√∫do dos documentos relevantes
                context = "\n\n".join([
                    f"Fonte: {d.metadata.get('source', 'desconhecido')}\nTrecho: {d.page_content[:1000]}"
                    for d in state['documents']
                ])

                # Se houver planilhas, adiciona instru√ß√µes sobre como lidar com dados tabulares
                planilha_present = any(d.metadata.get(
                    'tipo') == 'planilha' for d in state.get('documents', []))
                instrucoes_planilha = (
                    "Observa√ß√£o: os dados a seguir s√£o de uma planilha. "
                    "Considere os cabe√ßalhos e a estrutura tabular para responder com precis√£o."
                ) if planilha_present else ""

                # Monta o prompt final, incluindo contexto e pergunta
                prompt = f"""
Contexto (use APENAS os documentos carregados):
{context}

{instrucoes_planilha}

Pergunta: {question}
Resposta detalhada e precisa utilizando somente o contexto fornecido e indicando as fontes quando poss√≠vel:
"""
                resposta = chat_model.predict(prompt)
            else:
                # Se n√£o h√° documentos, gera resposta direta sem contexto
                resposta = chat_model.predict(
                    f"Responda √† pergunta de forma clara e direta: {question}"
                )

            # Armazena a resposta no hist√≥rico de mensagens e na mem√≥ria
            state['messages'].append(
                {"role": "assistant", "content": resposta}
            )
            state['memory'].save_context(
                {"input": question},
                {"output": resposta}
            )
        except Exception as e:
            resposta = f"‚ùå Erro na gera√ß√£o de resposta: {str(e)}"
            st.error(resposta)

        return state

    # Cria o grafo de estados do workflow
    workflow = StateGraph(AgentState)

    # Adiciona os n√≥s (fun√ß√µes) no workflow
    workflow.add_node("process_files", process_uploaded_files)
    workflow.add_node("handle_special_request", handle_special_request)
    workflow.add_node("retrieve_documents", retrieve_documents)
    workflow.add_node("generate_answer", generate_answer)

    # Define o ponto de entrada do fluxo
    workflow.set_entry_point("process_files")

    # Cria condi√ß√µes de transi√ß√£o entre os n√≥s
    workflow.add_conditional_edges(
        "process_files",
        route_question,
        {
            "handle_special_request": "handle_special_request",
            "retrieve_documents": "retrieve_documents",
            "generate_direct_answer": "generate_answer"
        }
    )

    workflow.add_edge("retrieve_documents", "generate_answer")
    workflow.add_edge("generate_answer", END)
    workflow.add_edge("handle_special_request", END)

    # Compila e retorna o workflow configurado
    return workflow.compile()


# Fun√ß√£o que transcreve √°udios usando Whisper (API da OpenAI).
# S√≥ transcreve √°udios de at√© 25MB.
def transcrever_audio(caminho_arquivo, nome_original, origin):
    try:
        # Verifica limite de tamanho
        if os.path.getsize(caminho_arquivo) > 25 * 1024 * 1024:
            return []
        # Abre o arquivo e solicita a transcri√ß√£o via API
        with open(caminho_arquivo, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                response_format="text",
                language="pt"
            )
        # Formata o texto transcrito e anonimiza dados sens√≠veis
        transcript = anonimizar_dados(
            re.sub(
                r'\n{3,}', '\n\n',
                re.sub(
                    r'([.!?])(\s+)', r'\1\n\n',
                    re.sub(r',(\s+)', r',\1', transcript)
                )
            )
        )
        return [Document(
            page_content=transcript,
            metadata={'source': nome_original,
                      'tipo': '√°udio', 'origin': origin}
        )]
    except Exception as e:
        st.error(f"Erro na transcri√ß√£o de √°udio: {str(e)}")
        return []


# Fun√ß√£o para "processar" imagem, basicamente chamando o modelo de chat
# para gerar uma descri√ß√£o detalhada do que est√° na imagem.
def processar_imagem(caminho_arquivo, origin):
    try:
        # Fun√ß√£o auxiliar para transformar a imagem em Base64
        def codificar_imagem(image_path):
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')

        selected_model = st.session_state.get("selected_model", "gpt-4o")

        # Monta a requisi√ß√£o para o Chat, enviando a imagem em Base64
        resposta = client.chat.completions.create(
            model=selected_model,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": "Descreva esta imagem detalhadamente considerando:"},
                    {"type": "text", "text": "1. Elementos visuais principais\n2. Texto leg√≠vel\n3. Contexto geral\n4. Detalhes relevantes\n5. Poss√≠veis significados"},
                    {"type": "image_url", "image_url": {
                        "url": f"data:image/jpeg;base64,{codificar_imagem(caminho_arquivo)}",
                        "detail": "high"
                    }},
                ]
            }],
            max_tokens=4000,
        )

        # Retorna um Document com a descri√ß√£o da imagem
        return [Document(
            page_content=anonimizar_dados(resposta.choices[0].message.content),
            metadata={'source': os.path.basename(caminho_arquivo),
                      'tipo': 'imagem', 'origin': origin}
        )]
    except Exception as e:
        st.error(f"Erro no processamento de imagem: {str(e)}")
        return []


# Fun√ß√£o auxiliar para exibir um relat√≥rio profissional de uma planilha,
# incluindo an√°lises estat√≠sticas e gr√°ficos gerados com Plotly.
def exibir_relatorio_planilha(data, filename):
    st.markdown(f"## Relat√≥rio Profissional da Planilha: {filename}")

    # Se 'data' for um DataFrame √∫nico
    if isinstance(data, pd.DataFrame):
        df = data.copy()

        # Mostra um resumo estat√≠stico das colunas
        st.markdown("### Resumo Estat√≠stico")
        summary = df.describe(include='all').transpose().reset_index()
        summary.columns = ["Vari√°vel"] + list(summary.columns[1:])

        # Cria uma tabela com as estat√≠sticas
        fig_summary = go.Figure(data=[go.Table(
            header=dict(
                values=list(summary.columns),
                fill_color='paleturquoise',
                align='left'
            ),
            cells=dict(
                values=[summary[col] for col in summary.columns],
                fill_color='lavender',
                align='left'
            )
        )])

        st.plotly_chart(fig_summary, use_container_width=True,
                        key=f"summary_{filename}")

        # Gera gr√°ficos de frequ√™ncia para colunas categ√≥ricas
        cat_cols = df.select_dtypes(include=['object', 'category']).columns
        for col in cat_cols:
            st.markdown(f"### Gr√°fico de Frequ√™ncia para {col}")
            freq = df[col].value_counts().reset_index()
            freq.columns = [col, 'Frequ√™ncia']
            fig_bar = px.bar(freq, x=col, y='Frequ√™ncia',
                             title=f"Frequ√™ncia de {col}")
            st.plotly_chart(fig_bar, use_container_width=True,
                            key=f"bar_{filename}_{col}")

        # Gera histogramas e gr√°ficos de tend√™ncia para colunas num√©ricas
        num_cols = df.select_dtypes(include='number').columns
        for col in num_cols:
            st.markdown(f"### Histograma para {col}")
            fig_hist = px.histogram(
                df, x=col, nbins=20, title=f"Histograma de {col}")
            st.plotly_chart(fig_hist, use_container_width=True,
                            key=f"hist_{filename}_{col}")

            st.markdown(f"### Gr√°fico de Tend√™ncia para {col}")
            fig_trend = px.line(df.reset_index(), x='index',
                                y=col, title=f"Tend√™ncia de {col}")
            st.plotly_chart(fig_trend, use_container_width=True,
                            key=f"trend_{filename}_{col}")

        # Se houver mais de uma coluna num√©rica, exibe a matriz de correla√ß√£o
        if len(num_cols) > 1:
            st.markdown("### Matriz de Correla√ß√£o")
            corr = df[num_cols].corr()
            fig_corr = px.imshow(corr, text_auto=True,
                                 title="Matriz de Correla√ß√£o")
            st.plotly_chart(fig_corr, use_container_width=True,
                            key=f"corr_{filename}")

    # Se 'data' for um dicion√°rio, assumimos que √© o caso de m√∫ltiplas sheets (XLSX)
    elif isinstance(data, dict):
        for sheet, df in data.items():
            st.markdown(f"## Relat√≥rio para Sheet: {sheet}")
            if not df.empty:
                exibir_relatorio_planilha(df, f"{filename}_{sheet}")
            else:
                st.info("Sheet vazia")


# Fun√ß√£o principal da aplica√ß√£o Streamlit
def main():
    st.title("Assistente Inteligente Multimodal üìöüé§üì∑ü§ñ")

    # Inicializa o workflow se ainda n√£o existir no estado
    if 'workflow' not in st.session_state:
        st.session_state.workflow = create_workflow()

    # Gera um ID √∫nico de sess√£o
    if 'session_id' not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())
    session_id = st.session_state.session_id

    # Dicion√°rio que armazena o estado do usu√°rio baseado no session_id
    if 'user_sessions' not in st.session_state:
        st.session_state.user_sessions = {}
    if session_id not in st.session_state.user_sessions:
        initial_state = {
            "messages": [],
            "vetorstore": None,
            "documents": [],
            "file_uploads": [],
            "memory": ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                input_key="input",
                output_key="output"
            ),
            "using_uploaded": False,
            "uploaded_dataframes": {}
        }
        st.session_state.user_sessions[session_id] = initial_state

    user_session = st.session_state.user_sessions[session_id]

    # Barra lateral para gerenciamento de conte√∫do
    with st.sidebar:
        st.header("üìÅ Gerenciamento de Conte√∫do")
        # Permite escolher o modelo de ChatOpenAI
        modelo_escolhido = st.selectbox("Selecione o modelo para respostas:",
                                        ["gpt-4o", "o1-mini"])
        st.session_state["selected_model"] = modelo_escolhido

        with st.expander("üìå Instru√ß√µes R√°pidas", expanded=True):
            st.markdown("""
            - Formatos suportados: Documentos, Imagens, √Åudios e Planilhas  
            - Tamanho m√°ximo para arquivo de √°udio: 25MB  
            - Carregar arquivos substitui o conhecimento atual
            """)

        # Upload de arquivos
        arquivos_enviados = st.file_uploader(
            "Carregar arquivos (arraste ou selecione)",
            type=EXTENSOES_PERMITIDAS,
            accept_multiple_files=True
        )

        # Se existir arquivo(s) enviado(s), atualiza o estado e processa
        if arquivos_enviados:
            user_session['file_uploads'] = arquivos_enviados
            if st.button("‚è≥ Processar Conte√∫do", use_container_width=True):
                with st.spinner("Processando arquivos..."):
                    st.session_state.workflow.invoke(user_session)

        # Se n√£o h√° arquivos novos e j√° houve upload antes, limpa a base
        elif user_session.get('using_uploaded', False):
            st.info(
                "Arquivos carregados foram fechados. A base de conhecimento est√° vazia."
            )
            user_session['vetorstore'] = None
            user_session['documents'] = []
            user_session['file_uploads'] = []
            user_session['using_uploaded'] = False
            user_session['uploaded_dataframes'] = {}

    # Exibe as mensagens trocadas at√© agora (hist√≥rico)
    for mensagem in user_session['messages']:
        with st.chat_message(mensagem["role"]):
            st.markdown(mensagem["content"])

    # √Årea principal do chat, onde o usu√°rio digita a pergunta
    with st.container():
        pergunta = st.chat_input(
            "üé§ Fa√ßa sua pergunta sobre documentos, imagens, √°udios ou planilhas..."
        )
        if pergunta:
            # Adiciona a pergunta ao hist√≥rico
            user_session['messages'].append(
                {"role": "user", "content": pergunta}
            )
            with st.chat_message("user"):
                st.markdown(pergunta)

            # Processa a pergunta usando o workflow
            try:
                with st.spinner("Processando sua pergunta..."):
                    for output in st.session_state.workflow.stream(user_session):
                        for key in output:
                            user_session.update(output[key])

                # Exibe a √∫ltima resposta gerada
                with st.chat_message("assistant"):
                    st.markdown(user_session['messages'][-1]['content'])

                    # Exibe as refer√™ncias (fontes) dos √∫ltimos documentos usados
                    if 'documents' in user_session and user_session['documents']:
                        with st.expander("üìö Fontes de Refer√™ncia"):
                            for i, doc in enumerate(user_session['documents'][-5:]):
                                nome = normalizar_nome_arquivo(
                                    doc.metadata['source'])
                                link = doc.metadata.get('link', '')
                                # Se houver link, exibe como link clic√°vel; caso contr√°rio, mostra somente o nome
                                st.markdown(
                                    f"**Documento {i+1}:** {'[üìé ' + nome + '](' + link + ')' if link else f'`{nome}`'}"
                                )
                                st.caption(
                                    f"Tipo: {doc.metadata.get('tipo', 'documento')}\n{doc.page_content[:300]}..."
                                )

            except Exception as e:
                st.error(f"‚ùå Erro na gera√ß√£o de resposta: {str(e)}")

            # Se a pergunta menciona "relat√≥rio" e "planilha", gera o relat√≥rio para as planilhas carregadas
            if re.search(r'relat[o√≥]rio', pergunta, re.IGNORECASE) and re.search(r'planilha', pergunta, re.IGNORECASE):
                if user_session.get('uploaded_dataframes'):
                    st.info(
                        "Gerando relat√≥rio profissional da(s) planilha(s) carregada(s)...")
                    for filename, data in user_session['uploaded_dataframes'].items():
                        exibir_relatorio_planilha(data, filename)
                else:
                    st.info("Nenhuma planilha carregada para gerar relat√≥rio.")

        # Bot√£o para limpar todo o hist√≥rico de conversas
        if st.button("üßπ Limpar Hist√≥rico", use_container_width=True):
            user_session['messages'] = []
            user_session['memory'].clear()
            user_session['using_uploaded'] = False
            user_session['uploaded_dataframes'] = {}
            st.rerun()


# Execu√ß√£o do Streamlit
if __name__ == "__main__":
    main()
